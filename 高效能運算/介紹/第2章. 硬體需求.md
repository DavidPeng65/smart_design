第2章. 硬體需求
---
### 1.1 HPC 伺服器與工作站的差異解析
HPC（高效能計算）伺服器與工作站之間有一些顯著的差異，它們主要體現在應用場景、硬體配置以及所支援的運算能力等方面。以下是它們的主要差異：
1. **應用場景**  
- **HPC 伺服器** ：通常用於需要極高計算能力的科學、工程、金融模擬等應用場景，例如流體動力學（CFD）、電磁場模擬、基因組分析、大數據分析等。這些應用需要並行計算的大量處理器資源，常常需要多個伺服器集群協作運算。
 
- **工作站** ：工作站通常用於單個使用者的高性能需求，例如 CAD 設計、工程分析、視覺效果製作等。它們雖然也具備強大的運算能力，但更多是針對高效能互動和圖形處理需求。
2. **硬體配置**  
- **HPC 伺服器** ： 
  - **處理器** ：通常配備多個處理器（CPUs），支援大量核心和並行計算。可擴展的硬體架構支援集群中的多節點協同工作。
 
  - **記憶體** ：擁有大量的 RAM，以支援處理超大規模的數據集和計算需求。常見有 Terabyte 等級的 RAM 記憶體。
 
  - **儲存** ：通常使用高速的分佈式儲存系統（如 Lustre、GPFS），以支援高吞吐量的讀寫操作。
 
  - **網路** ：高帶寬、低延遲的網路連接（如 InfiniBand），以加速節點間數據傳輸。
 
- **工作站** ： 
  - **處理器** ：配備單個或雙處理器，且擁有多核心，但相較於 HPC 伺服器，其擴展性有限。
 
  - **記憶體** ：擁有足夠的 RAM（通常在數十 GB 到數百 GB），以支援高性能應用。
 
  - **儲存** ：通常採用本地化高速 SSD，且較為著重資料的存取速度和工作效能。
 
  - **網路** ：通常沒有專門的高帶寬網路，主要是標準的以太網，用於一般的網路通訊需求。
3. **計算能力**  
- **HPC 伺服器** ：
  - 可組建成數百甚至數千個節點的集群，每個節點有多個 CPU/GPU，能提供龐大的併行計算能力。

  - 適合需要長時間、多節點合作進行的大型科學與工程計算。
 
- **工作站** ：
  - 提供足夠的計算能力來處理高要求的應用程式，但通常是針對單一使用者和單一節點的需求。

  - 專注於高互動性和圖形顯示，如 3D 渲染或設計軟體的應用場景。
4. **散熱與能耗**  
- **HPC 伺服器** ：由於具備龐大的處理能力，因此也伴隨著極高的功耗和散熱需求。HPC 通常配備高效的冷卻系統，甚至可能需要液冷或專用冷卻設施。
 
- **工作站** ：相對來說，工作站的能耗較低，其散熱系統也較為簡單，通常依賴風冷即可。
5. **擴展性**  
- **HPC 伺服器** ：設計上具有高度的擴展性，可以根據需求擴增計算節點和存儲設備，輕鬆組建大型計算集群。
 
- **工作站** ：擴展性相對有限，雖然可以增加內存或更換顯卡，但無法達到 HPC 集群那樣的水平。
6. **軟體生態與管理**  
- **HPC 伺服器** ：通常使用 Linux 為主的操作系統，並且需配置特殊的資源管理與併行計算軟體（如 Slurm、PBS），來分配計算資源並管理工作負載。
 
- **工作站** ：可以安裝 Windows 或 Linux，一般更傾向於安裝用戶友好的桌面操作系統，針對特定的應用程序進行優化。

### 總結 

HPC 伺服器和工作站在硬體配置、應用場景和擴展性等方面都有很大的差異。HPC 伺服器適合於需要高性能並行計算和大量數據處理的應用，而工作站則更加適合單一使用者在互動性和計算需求之間取得平衡的工作環境。

選擇使用哪種設備應取決於具體的計算需求、應用類型以及預算考量。例如，如果需要進行複雜的大型多物理場仿真，HPC 伺服器會是更合適的選擇；但如果只是進行設計和有限元分析，且需要高互動性，那麼工作站便足夠應付需求。

### 1.2 高帶寬、低延遲網路
在高效能計算（HPC）環境中，節點之間的網路配置對於整體性能至關重要，尤其是涉及大量併行計算和數據交換的情況時。HPC 系統通常需要高度可靠且低延遲的網絡來實現節點間快速而高效的數據通信。下面我將介紹 HPC 系統中適合的高速網路配置，涵蓋了插槽、網卡、網線、交換器等各種關鍵組件。
#### **1. 插槽選擇**  
- **PCIe（Peripheral Component Interconnect Express）插槽** ： 
  - **PCIe Gen4 或 Gen5**  插槽是目前 HPC 伺服器中最常見的擴展插槽選擇。這些插槽提供高達 **256 Gbps** （PCIe Gen4 x16）和更高（PCIe Gen5 x16）的帶寬，非常適合高速網卡（NIC，Network Interface Card）等擴展設備。

  - 在 HPC 中，插槽的帶寬至關重要，因為它決定了網卡可以支持的最大數據傳輸速率。因此，選擇 PCIe Gen4 或 Gen5 可以確保能夠滿足高網絡吞吐量的需求。
 
- **OCP 3.0 插槽** ： 
  - **OCP 3.0** （Open Compute Project 3.0）插槽通常被用於擴展伺服器的網卡，特別是在需要高帶寬且不佔用標準 PCIe 插槽時。OCP 3.0 可以支持 **25GbE、40GbE、100GbE**  的網卡，是一種節省空間且便於熱插拔的擴展方式，適合高密度的 HPC 伺服器配置。

#### **2. 高速網卡（NIC）**  
- **InfiniBand 網卡** ： 
  - **InfiniBand**  是 HPC 中最常用的網路協議之一，提供超高的帶寬（例如 **100 Gbps 到 400 Gbps** ）和極低的延遲（通常小於 **1 微秒** ）。InfiniBand 網卡是通過 PCIe 插槽連接的，支持遠端直接記憶體訪問（RDMA），這可以顯著減少 CPU 的負擔並提高數據交換效率。

  - InfiniBand 特別適合於需要頻繁多節點之間通信的 HPC 應用，如流體動力學、氣象模擬等科學計算任務。
 
- **以太網網卡（Ethernet NIC）** ： 
  - 在 HPC 中，高速以太網網卡也是一種常見選擇。隨著 **25GbE、40GbE、100GbE**  甚至 **400GbE**  以太網標準的出現，以太網已經可以在一些 HPC 應用中提供足夠的性能。
 
  - **RoCE（RDMA over Converged Ethernet）**  是以太網技術中的一種增強協議，它可以模擬 RDMA 的功能，降低以太網的延遲，提高數據傳輸效率，使得以太網可以與 InfiniBand 競爭。

  - 這些網卡可以通過 PCIe 或 OCP 3.0 插槽進行擴展，以提供更多節點之間的高速通信。

#### **3. 網線（Cabling）**  
- **光纖線纜** ： 
  - 在高帶寬和長距離通信場景中，**光纖線纜**  是最佳選擇。光纖可以提供從 **25Gbps 到 400Gbps**  的傳輸速率，並且具有低延遲和良好的抗干擾能力，適合在數據中心內連接不同伺服器節點或交換器。
 
  - 光纖的連接頭常用 **QSFP** （Quad Small Form-factor Pluggable）標準，這些光纖模組可以直接插入網卡或交換器的端口中。
 
- **DAC（Direct Attach Copper）銅纜** ： 
  - **DAC 線纜**  是短距離連接的一種經濟選擇。它使用銅纜來進行數據傳輸，通常適合於伺服器與交換器之間的連接，距離一般不超過 7 米。
 
  - DAC 具備 QSFP 接頭，能夠提供從 **10Gbps 到 100Gbps**  的傳輸速率，並且價格比光纖便宜，是在高效能計算機房內短距離連接的常見選擇。
 
- **AOC（Active Optical Cable）** ： 
  - **AOC**  結合了銅纜和光纖的特點，用於短距離連接，提供了高達 **100Gbps**  的速率。它比光纖容易管理且相對便宜，適合需要更高速度但距離不長的應用場景。

####  **4. 高速交換器（Switches）**  
- **InfiniBand 交換器** ： 
  - **InfiniBand 交換器**  是為 HPC 設計的專用交換器，能夠處理超高頻的數據交換。這些交換器支持從 **100 Gbps 到 400 Gbps**  的連接速率，能夠在集群節點之間提供低延遲、高吞吐量的數據傳輸。
 
  - **Mellanox**  是 InfiniBand 交換器的主要供應商之一，提供多層級的交換器方案，適用於從小型集群到超大型超級計算機。
 
- **高性能以太網交換器** ： 
  - **以太網交換器**  的高速型號（例如 **100GbE 或 400GbE**  交換器）也被用於 HPC 集群中，特別是在希望使用更熟悉的以太網技術並且成本控制較為重要的場景下。
 
  - 這些交換器通常會配備 **RoCE**  技術來支持 RDMA，以降低網路延遲，從而在多節點併行計算中達到和 InfiniBand 類似的性能。
 
  - 廠商如 **Cisco** 、**Arista**  和 **Juniper**  提供了大量支持高帶寬和低延遲的交換器解決方案，適合各種不同規模的 HPC 網絡配置。

#### **5. RDMA 技術**  
- **RDMA（Remote Direct Memory Access）**  是高效能計算中用於降低網路延遲並減少 CPU 負擔的一種重要技術。 
  - **InfiniBand**  和 **RoCE（RDMA over Converged Ethernet）**  都支持 RDMA，這使得節點之間可以直接訪問彼此的內存，而不需要經過 CPU 處理，大幅度提高了數據通信的效率和性能。

  - 在 HPC 環境中，RDMA 能顯著降低多節點之間通信時的延遲，從而提高整個集群的併行效率。

#### **結論：HPC 適合的高速網路配置** 
在 HPC 中，選擇適合的網路配置需要根據工作負載的特性、集群規模以及預算等因素來決定。以下是一般推薦的高速網路配置：
 
1. **插槽** ： 
  - **PCIe Gen4/Gen5**  或 **OCP 3.0**  插槽，用於擴展高速網卡。
 
2. **網卡** ： 
  - **InfiniBand 網卡** （如 HDR 100 Gbps 或 NDR 400 Gbps）適合需要極低延遲和高帶寬的場景。
 
  - **高速以太網網卡** （如 25GbE、40GbE、100GbE）適合有預算考量的高效能應用，可以配合 RoCE 技術以降低延遲。
 
3. **網線** ： 
  - **光纖線纜** （QSFP+）適合長距離、高頻寬連接。
 
  - **DAC 銅纜**  適合短距離伺服器與交換器之間的連接。
 
4. **交換器** ： 
  - **InfiniBand 交換器** （如 Mellanox 交換器）適合需要最高性能的 HPC 集群。
 
  - **高性能以太網交換器** （如 100GbE/400GbE 交換器）則可作為成本效益更好的選擇，特別是對於需要用到 RoCE 技術的集群。
 
5. **RDMA 支持** ： 
  - 使用 **InfiniBand**  或 **RoCE**  以確保節點之間的低延遲和高效數據交換，這是 HPC 網絡中達成高性能的關鍵。

這樣的配置能夠保證 HPC 集群的各個節點之間可以高效、快速地進行數據交換，最大程度上提高計算性能和集群效率。

